# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.
Original file: located at
    https://colab.research.google.com/drive/1J7avAYXS7leS-V8sf7t-1-dibZA-XgJo

Hate Speech Detection System In Indonesian Language Using
Indonesian Bidirectional Encoder Representations from
Transformers (IndoBERT) Method

By Rizky Anugerah
"""

"""Import the libraries."""

import os
import pickle
import random
import re
import time
import timeit

import matplotlib.pyplot as plt
import nltk
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import torch
import torch.nn.functional as F
from imblearn.over_sampling import SMOTE
from indonlu.utils.data_utils import (
    HateSpeechClassificationDataLoader,
    MultiLabelHateSpeechClassificationDataset,
)
from indonlu.utils.forward_fn import forward_sequence_classification
from indonlu.utils.metrics import multi_label_hate_speech_classification_metrics_fn
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import train_test_split
from torch import optim
from tqdm import tqdm
from transformers import AdamW, BertConfig, BertForSequenceClassification, BertTokenizer

"""
Set global variable, parameter, and hyperparameter used (Hyperparameter Tuning).
"""

batch_size = 32  # batch size
lr = 3e-5  # learning rate
epochs = 1  # epochs
attention_probs_dropout_prob = 0.1  # dropout
hidden_dropout_prob = 0.1  # dropout

max_seq_len = 512  # maximum length
num_labels = 4  # number of labels
num_workers = (
    0  # number of workers // lower the worker number to avoid potential slowness/freeze
)
eps = 1e-8  # epsilon
weight_decay = 1e-2  # weight decay

random_state = 0  # random_state


def set_seed(seed):
    """Set random seed."""

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)


def count_param(module, trainable=False):
    """Count number of parameters."""

    if trainable:
        return sum(p.numel() for p in module.parameters() if p.requires_grad)
    else:
        return sum(p.numel() for p in module.parameters())


def get_lr(optimizer):
    """Get the learning rate."""

    for param_group in optimizer.param_groups:
        return param_group["lr"]


def metrics_to_string(metric_dict):
    """Change from metrics evaluation to string."""

    string_list = []

    for key, value in metric_dict.items():
        string_list.append("{}:{:.2f}".format(key, value))
    return " ".join(string_list)


def show_histogram(df):
    """Show data on Histogram."""

    pd.value_counts(df["HS"]).plot.bar()
    plt.title("Label Comparison")
    plt.xlabel("Label")
    plt.ylabel("Count")

    # If you don't want to save this chart, comment line below
    plt.savefig("../data/output/image/histogram", dpi=300)

    df["HS"].value_counts()


def show_pie(inner_label, outer_label, inner_df, outer_df, legend_title):
    """Show data on Donut Chart."""

    print("Rendering chart...")
    for i in tqdm(range(0, 100), desc="Rendering"):
        time.sleep(0.03)

    fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(aspect="equal"))
    width = 0.3
    labels = [x.split()[-1] for x in inner_label]

    def func(pct, allvals):
        absolute = int(round(pct / 100.0 * np.sum(allvals)))
        return "{:.1f}% ({:d})".format(pct, absolute)

    # inner pie
    inner_pie = ax.pie(
        inner_df,
        autopct=lambda pct: func(pct, inner_df),
        textprops=dict(color="white", weight="bold"),
        pctdistance=0.55,
        wedgeprops={"width": 0.6, "edgecolor": "white"},
    )
    plt.setp(inner_pie)

    # outer pie
    outer_pie = ax.pie(
        outer_df,
        autopct=lambda pct: func(pct, outer_df),
        textprops=dict(color="white", weight="bold"),
        pctdistance=0.85,
        wedgeprops={"width": 0.3, "edgecolor": "white"},
    )
    plt.setp(outer_pie)

    # set legend label
    ax.legend(
        labels, title=legend_title, loc="center left", bbox_to_anchor=(1, 0, 0.5, 1)
    )
    ax.set_title("Label Comparison")

    plt.tight_layout()

    # If you don't want to show this chart, comment line below
    # plt.show()

    # If you don't want to save this chart, comment line below
    plt.savefig("../data/output/image/donut_chart", dpi=300)


def text_preprocessing(df):
    """Text Preprocessing."""

    print("\nRunning Text Preprocessing...")

    # Import Slang Dictionary from (Ibrohim & Budi, 2019)
    alay_dict = "https://raw.githubusercontent.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection/master/new_kamusalay.csv"
    alay_dict = pd.read_csv(alay_dict, encoding="latin-1", header=None)
    alay_dict = alay_dict.rename(columns={0: "original", 1: "replacement"})

    print("\n[Slang Dictionary successfully imported]\n")
    print("--- Slang Dictionary Table ---")
    print(alay_dict.head())
    print("Number of Data: ", alay_dict.shape)

    # Data Cleaning
    print("\n[=== Data Cleaning ===]")
    print("\nRunning Data Cleaning...")

    nltk.download("stopwords")
    nltk.download("punkt")

    stop_words = nltk.corpus.stopwords.words("indonesian")

    def data_cleaning(text):
        """Data Cleaning."""

        emoticon_byte_regex = r"\s*(?:\\x[A-Fa-f0-9]{2})+"
        url_regex = "((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+)||(http\S+))"

        text = re.sub(emoticon_byte_regex, "", text)  # Remove emoticon bytes
        text = re.sub(url_regex, "", text)  # Remove every url
        text = re.sub(r"<[^>]*>", "", text)  # Remove html tags
        text = re.sub(r"@[A-Za-z0-9]+", "", text)  # Remove twitter usernames
        text = re.sub(r"\\n", " ", text)  # Remove every new line '\n'
        text = re.sub("@[\w\-]+", "", text)  # Remove mentions
        text = re.sub("RT", "", text)  # Remove every retweet symbol
        text = re.sub("USER", "", text)  # Remove every user
        text = re.sub(" URL", " ", text)  # Remove word URL
        text = re.sub(" url", " ", text)  # Remove word url
        text = re.sub("\\+", " ", text)  # Remove backslash
        text = re.sub("\s+", " ", text)  # Remove special regular expression character
        text = re.sub("[^0-9a-zA-Z]", " ", text)  # Remove punctuation
        text = re.sub("[^a-zA-Z]", " ", text)  # Remove numbers
        text = re.sub(" +", " ", text)  # Remove extra spaces

        word_tokens = word_tokenize(text)

        filtered_sentence = []
        for word_token in word_tokens:
            if word_token not in stop_words:
                filtered_sentence.append(word_token)

        # Join words
        text = " ".join(filtered_sentence)

        return text

    df["text_cleaned"] = df["Tweet"].apply(data_cleaning)
    df.text_cleaned = df.text_cleaned.str.strip()

    print("\n[Dataset successfully cleaned]\n")
    print("--- Data Cleaned Table ---")
    print(df[["Tweet", "text_cleaned"]].head())

    # Case Folding
    print("\n[=== Case Folding ===]")
    print("\nRunning Case Folding...")

    def case_folding(text):
        """Lowercase letters."""

        text = text.lower()
        return text

    df["text_folded"] = df["text_cleaned"].apply(case_folding)

    print("\n[Dataset successfully folded]\n")
    print("--- Case Folded Table ---")
    print(df[["text_cleaned", "text_folded"]].head())

    # Normalization
    print("[=== Normalization ===]")
    print("\nRunning Normalization...")

    alay_dict_map = dict(zip(alay_dict["original"], alay_dict["replacement"]))

    def normalize_alay_dict(text):
        """Normalization."""

        return " ".join(
            [
                alay_dict_map[word] if word in alay_dict_map else word
                for word in text.split(" ")
            ]
        )

    df["text_normalized"] = df["text_folded"].apply(normalize_alay_dict)

    print("\n[Dataset successfully normalized]\n")
    print("--- Data Normalized Table ---")
    print(df[["text_folded", "text_normalized"]].head())

    # Filtering
    print("\n[=== Filtering ===]")
    print("\nRunning Filtering...")

    # Use stopword from Tala, F. Z. (2003)
    id_stopword_dict = pd.read_csv(
        "https://raw.githubusercontent.com/stopwords-iso/stopwords-id/master/stopwords-id.txt",
        header=None,
    )
    id_stopword_dict = id_stopword_dict.rename(columns={0: "stopword"})

    print("\n[Stopword successfully imported]\n")
    print("--- Stopword List ---")
    print(id_stopword_dict.head(10))

    def remove_stopword(text):
        """Remove Stopword/Filtering."""

        text = " ".join(
            [
                "" if word in id_stopword_dict.stopword.values else word
                for word in text.split(" ")
            ]
        )
        text = re.sub("  +", " ", text)  # remove extra spaces
        text = text.strip()

        return text

    df["text_filtered"] = df["text_normalized"].apply(remove_stopword)
    print("\n[Dataset successfully filtered]\n")
    print("--- Data Filtered Table ---")
    print(df[["text_normalized", "text_filtered"]].head())

    # Stemming
    print("\n[=== Stemming ===]")
    print("\nRunning Stemming...")

    # Create stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()

    def stemming(text):
        """Stemming."""

        return stemmer.stem(text)

    df["text"] = df["text_filtered"].apply(stemming)
    print("\n[Dataset successfully stemmed]\n")
    print("--- Data Stemmed Table ---")

    df = df[["text_filtered", "text"]]
    print(df.head())

    return df


def index_classification(df):
    """Index Classification."""

    print("Running index classification...")
    for i in tqdm(range(0, 100), desc="Running"):
        time.sleep(0.03)
    print("\n[Dataset successfully indexed]\n")

    def index_classification_weak(hs):
        """Index Classification for weak hate speech."""

        label = ""

        if int(hs) == 1:
            label = 1
        else:
            label = 0

        return label

    def index_classification_moderate(hs):
        """Index Classification for moderate hate speech."""

        label = ""

        if int(hs) == 1:
            label = 2
        else:
            label = 0

        return label

    def index_classification_strong(hs):
        """Index Classification for strong hate speech."""

        label = ""

        if int(hs) == 1:
            label = 3
        else:
            label = 0

        return label

    df["label_weak"] = df["HS_Weak"].apply(index_classification_weak)
    df["label_moderate"] = df["HS_Moderate"].apply(index_classification_moderate)
    df["label_strong"] = df["HS_Strong"].apply(index_classification_strong)

    df["label"] = df[["label_weak", "label_moderate", "label_strong"]].max(1)
    df = df[["text", "label"]]

    df.to_csv("../data/output/dataset/dataset_preprocessed.csv", index=False)

    print("--- Data Indexed Table ---")
    print(df.head())
    print("Number of Data: ", df.shape)

    # Converting to df and assigning new name to the columns
    value_counts = df["label"].value_counts()

    df_value_counts = pd.DataFrame(value_counts)
    df_value_counts = df_value_counts.reset_index()
    df_value_counts.columns = ["label", "counts of label"]  # Change columns name

    print("\n--- Counts Of Label Table ---")
    print(df_value_counts)


def smote(df_train, df_valid, df_test, y_train, y_valid, y_test):
    """Balancing Data with SMOTE: Synthetic Minority Over-sampling Technique (Optional)."""

    print("\n[================================================]")
    print("\nRunning SMOTE...")

    # Convert strings into numericals using TfidfVectorizer
    vec_train = TfidfVectorizer()
    vec_test = TfidfVectorizer()
    vec_valid = TfidfVectorizer()

    X_train = vec_train.fit_transform(df_train["text"])
    X_valid = vec_valid.fit_transform(df_valid["text"])
    X_test = vec_test.fit_transform(df_test["text"])

    sm = SMOTE(random_state=random_state)

    X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())
    X_test_res, y_test_res = sm.fit_resample(X_test, y_test.ravel())
    X_valid_res, y_valid_res = sm.fit_resample(X_valid, y_valid.ravel())

    # Convert back into strings using inverse_transform
    X_train_res = vec_train.inverse_transform(X_train_res)
    X_test_res = vec_test.inverse_transform(X_test_res)
    X_valid_res = vec_valid.inverse_transform(X_valid_res)

    pd.DataFrame({"text": X_train_res}).to_csv(
        "../data/output/dataset/smote/X_train.csv", index=False
    )
    pd.DataFrame({"label": y_train_res}).to_csv(
        "../data/output/dataset/smote/y_train.csv", index=False
    )

    pd.DataFrame({"text": X_test_res}).to_csv(
        "../data/output/dataset/smote/X_test.csv", index=False
    )
    pd.DataFrame({"label": y_test_res}).to_csv(
        "../data/output/dataset/smote/y_test.csv", index=False
    )

    pd.DataFrame({"text": X_valid_res}).to_csv(
        "../data/output/dataset/smote/X_valid.csv", index=False
    )
    pd.DataFrame({"label": y_valid_res}).to_csv(
        "../data/output/dataset/smote/y_valid.csv", index=False
    )

    # Show counts of training dataset label
    print("\n--- Info Train Dataset ---")
    print("Before OverSampling, counts of label '0': {}".format(sum(y_train == 0)))
    print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
    print("Before OverSampling, counts of label '2': {}".format(sum(y_train == 2)))
    print("Before OverSampling, counts of label '3': {}".format(sum(y_train == 3)))

    print("\nAfter OverSampling, counts of label '0': {}".format(sum(y_train_res == 0)))
    print("After OverSampling, counts of label '1': {}".format(sum(y_train_res == 1)))
    print("After OverSampling, counts of label '2': {}".format(sum(y_train_res == 2)))
    print("After OverSampling, counts of label '3': {}".format(sum(y_train_res == 3)))

    print("\nAfter OverSampling, the shape of train_y: {}".format(y_train_res.shape))

    # Show counts of validation dataset label
    print("\n--- Info Valid Dataset ---")
    print("Before OverSampling, counts of label '0': {}".format(sum(y_valid == 0)))
    print("Before OverSampling, counts of label '1': {}".format(sum(y_valid == 1)))
    print("Before OverSampling, counts of label '2': {}".format(sum(y_valid == 2)))
    print("Before OverSampling, counts of label '3': {}".format(sum(y_valid == 3)))

    print("\nAfter OverSampling, counts of label '0': {}".format(sum(y_valid_res == 0)))
    print("After OverSampling, counts of label '1': {}".format(sum(y_valid_res == 1)))
    print("After OverSampling, counts of label '2': {}".format(sum(y_valid_res == 2)))
    print("After OverSampling, counts of label '3': {}".format(sum(y_valid_res == 3)))

    print("\nAfter OverSampling, the shape of valid_y: {}".format(y_valid_res.shape))

    # Show counts of testing dataset label
    print("\n--- Info Test Dataset ---")
    print("Before OverSampling, counts of label '0': {}".format(sum(y_test == 0)))
    print("Before OverSampling, counts of label '1': {}".format(sum(y_test == 1)))
    print("Before OverSampling, counts of label '2': {}".format(sum(y_test == 2)))
    print("Before OverSampling, counts of label '3': {}".format(sum(y_test == 3)))

    print("\nAfter OverSampling, counts of label '0': {}".format(sum(y_test_res == 0)))
    print("After OverSampling, counts of label '1': {}".format(sum(y_test_res == 1)))
    print("After OverSampling, counts of label '2': {}".format(sum(y_test_res == 2)))
    print("After OverSampling, counts of label '3': {}".format(sum(y_test_res == 3)))

    print("\nAfter OverSampling, the shape of test_y: {}".format(y_test_res.shape))

    # Clean the dataset to make sure data: clean
    def data_cleaning(text):
        text = re.sub("['']", "", text)
        text = text.replace("[", "").replace("]", "")
        return text

    df_train = pd.read_csv("../data/output/dataset/smote/X_train.csv")
    df_valid = pd.read_csv("../data/output/dataset/smote/X_valid.csv")
    df_test = pd.read_csv("../data/output/dataset/smote/X_test.csv")

    df_train["text"] = df_train["text"].apply(data_cleaning)
    df_train.text = df_train.text.str.strip()

    df_valid["text"] = df_valid["text"].apply(data_cleaning)
    df_valid.text = df_valid.text.str.strip()

    df_test["text"] = df_test["text"].apply(data_cleaning)
    df_test.text = df_test.text.str.strip()

    print("\n--- Data Train Table ---")
    print(df_train[["text"]].head())

    print("\n--- Data Valid Table ---")
    print(df_valid[["text"]].head())

    print("\n--- Data Test Table ---")
    print(df_test[["text"]].head())

    df_train.to_csv("../data/output/dataset/smote/X_train.csv", index=False)
    df_valid.to_csv("../data/output/dataset/smote/X_valid.csv", index=False)
    df_test.to_csv("../data/output/dataset/smote/X_test.csv", index=False)

    # save temp data training
    df_X_train = pd.read_csv("../data/output/dataset/smote/X_train.csv")
    df_y_train = pd.read_csv("../data/output/dataset/smote/y_train.csv")

    df_train = pd.concat([df_X_train, df_y_train], axis=1)

    # save temp data validation
    df_X_valid = pd.read_csv("../data/output/dataset/smote/X_valid.csv")
    df_y_valid = pd.read_csv("../data/output/dataset/smote/y_valid.csv")

    df_valid = pd.concat([df_X_valid, df_y_valid], axis=1)

    # save temp data testing
    df_X_test = pd.read_csv("../data/output/dataset/smote/X_test.csv")
    df_y_test = pd.read_csv("../data/output/dataset/smote/y_test.csv")

    df_test = pd.concat([df_X_test, df_y_test], axis=1)

    return df_train, df_valid, df_test


def split_dataset():
    """Split Up Dataset."""

    # Import preprocessed dataset
    print("Importing dataset...")
    for i in tqdm(range(0, 100), desc="Importing"):
        time.sleep(0.02)

    df = pd.read_csv("../data/output/dataset/dataset_preprocessed.csv")
    print("\n[Dataset successfully imported]")

    print("\n--- Dataset Preprocessed ---")
    print(df.head())
    print("Number of Data: ", df.shape)

    """
    Hold-out Validation.

    Split up dataset into train, valid, and test set
    with a ratio of 80% train, 10% test, and 10% valid.
    """

    # Define dataset
    X = df["text"]
    y = df["label"]

    # Split into 80:10:10 ration
    print("\nSplitting dataset with Hold-out Validation...")
    for i in tqdm(range(0, 100), desc="Splitting dataset"):
        time.sleep(0.05)

    X_train, X_rem, y_train, y_rem = train_test_split(
        X, y, train_size=0.8, random_state=random_state
    )
    X_valid, X_test, y_valid, y_test = train_test_split(
        X_rem, y_rem, test_size=0.5, random_state=random_state
    )

    print("\n[Dataset successfully splitted]")

    # Describe info about train, valid, and test set
    print("\n--- Train Dataset ---")
    print(y_train.value_counts())
    print("Number of Train Dataset: ", y_train.shape)

    print("\n--- Valid Dataset ---")
    print(y_valid.value_counts())
    print("Number of Valid Dataset: ", y_valid.shape)

    print("\n--- Test Dataset ---")
    print(y_test.value_counts())
    print("Number of Test Dataset: ", y_test.shape)

    df_train = pd.concat([X_train, y_train], axis=1)
    df_valid = pd.concat([X_valid, y_valid], axis=1)
    df_test = pd.concat([X_test, y_test], axis=1)

    df_train.to_csv("../data/output/dataset/dataset_training.csv", index=False)
    df_valid.to_csv("../data/output/dataset/dataset_validation.csv", index=False)
    df_test.to_csv("../data/output/dataset/dataset_testing.csv", index=False)

    # Uncomment line below if you want to work with balance data.
    # smote(df_train, df_valid, df_test, y_train, y_valid, y_test)

    """Label Classification."""

    # Data training
    def label_classification(hs):
        label = ""

        if int(hs) == 1:
            label = "HS_Weak"
        elif int(hs) == 2:
            label = "HS_Moderate"
        elif int(hs) == 3:
            label = "HS_Strong"
        else:
            label = "Non_HS"

        return label

    df_train["label"] = df_train["label"].apply(label_classification)
    df_train = df_train[["text", "label"]]
    df_train.to_csv("../data/output/dataset/dataset_training.csv", index=False)

    # Data validation
    def label_classification(hs):
        label = ""

        if int(hs) == 1:
            label = "HS_Weak"
        elif int(hs) == 2:
            label = "HS_Moderate"
        elif int(hs) == 3:
            label = "HS_Strong"
        else:
            label = "Non_HS"

        return label

    df_valid["label"] = df_valid["label"].apply(label_classification)
    df_valid = df_valid[["text", "label"]]
    df_valid.to_csv("../data/output/dataset/dataset_validation.csv", index=False)

    # Data testing
    def label_classification(hs):
        label = ""

        if int(hs) == 1:
            label = "HS_Weak"
        elif int(hs) == 2:
            label = "HS_Moderate"
        elif int(hs) == 3:
            label = "HS_Strong"
        else:
            label = "Non_HS"

        return label

    df_test["label"] = df_test["label"].apply(label_classification)
    df_test = df_test[["text", "label"]]
    df_test.to_csv("../data/output/dataset/dataset_testing.csv", index=False)

    """Remove Missing Values"""

    print("\n[================================================]")
    print("\nRemoving missing values...")

    # Remove missing value if any, just for make sure once again
    print("\n--- Before removing missing values ---")
    df_train = pd.read_csv("../data/output/dataset/dataset_training.csv")
    print(df_train.isnull().sum())

    df_test = pd.read_csv("../data/output/dataset/dataset_testing.csv")
    print(df_test.isnull().sum())

    df_valid = pd.read_csv("../data/output/dataset/dataset_validation.csv")
    print(df_valid.isnull().sum())

    print("\n--- After removing missing values ---")
    df_train = df_train[df_train["text"].notna()]
    print(df_train.isnull().sum())

    df_test = df_test[df_test["text"].notna()]
    print(df_test.isnull().sum())

    df_valid = df_valid[df_valid["text"].notna()]
    print(df_valid.isnull().sum())

    print("\nNumber of Data Training: ", df_train.shape)
    print("Number of Data Validation: ", df_valid.shape)
    print("Number of Data Testing: ", df_test.shape)

    # Save split up dataset
    print("\nDownloading dataset...")
    for i in tqdm(range(0, 100), desc="Downloading"):
        time.sleep(0.03)

    df_train.to_csv("../data/output/dataset/dataset_training.csv", index=False)
    df_test.to_csv("../data/output/dataset/dataset_testing.csv", index=False)
    df_valid.to_csv("../data/output/dataset/dataset_validation.csv", index=False)

    print("\n[Dataset successfully downloaded]")


def fine_tuning():
    """
    Modeling Pretrained Model (Training Model).
    """

    global i2w, model, tokenizer, testing_loader

    """Load tokenizer, config, and IndoBERT pretrained model."""

    # Load tokenizer
    print("Loading BERT tokenizer...")
    for i in tqdm(range(0, 100), desc="Loading"):
        time.sleep(0.02)
    tokenizer = BertTokenizer.from_pretrained("indobenchmark/indobert-base-p1")

    # Load config
    print("\nLoading BERT config...")
    for i in tqdm(range(0, 100), desc="Loading"):
        time.sleep(0.02)
    config = BertConfig.from_pretrained(
        "indobenchmark/indobert-base-p1",
        attention_probs_dropout_prob=attention_probs_dropout_prob,
        hidden_dropout_prob=hidden_dropout_prob,
        num_labels=num_labels,
    )
    config.num_labels = MultiLabelHateSpeechClassificationDataset.NUM_LABELS

    # Instantiate model
    print("\nDownloading BERT model...")
    for i in tqdm(range(0, 100), desc="Downloading"):
        time.sleep(0.02)
    model = BertForSequenceClassification.from_pretrained(
        "indobenchmark/indobert-base-p1", config=config
    )

    # Tell pytorch to run this model on the GPU
    model.cuda()

    # Get all of the model's parameters as a list of tuples.
    params = list(model.named_parameters())

    print("The BERT model has {:} different named parameters.\n".format(len(params)))

    print("==== Embedding Layer ====\n")

    for p in params[0:5]:
        print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

    print("\n==== First Transformer ====\n")

    for p in params[5:21]:
        print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

    print("\n==== Output Layer ====\n")

    for p in params[-4:]:
        print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

    config
    model

    count_param(model)

    """Apply the tokenizer to one sentence just to see the output."""

    print("\nApplying the tokenizer to one sentence...\n")

    text_1 = "kebahagiaan terbesarku adalah melihatmu bersama dengan dirinya"
    text_2 = "dengan begitu kau akan tenang bersamanya"

    text = (text_1, text_2)

    text = " ".join(text)

    token = tokenizer.tokenize(text)  # Tokenizing
    encoding = tokenizer.encode(text_1, text_2)  # Token ids
    decoding = tokenizer.decode(encoding)  # Token embeddings
    encoding_input = tokenizer(text)

    print("Text: ", text)
    print("Tokenized: ", token)
    print("Token Embeddings: ", decoding)
    print("Token IDs: ", encoding_input)

    """
    Create DataLoader.

    Create an iterator for dataset using Torch's DataLoader class.
    This helps save memory during training and don't have to load the entire dataset into memory.
    """

    print("\nCreating DataLoader...")
    for i in tqdm(range(0, 100), desc="Creating DataLoader"):
        time.sleep(0.04)

    training_dataset_path = "../data/output/dataset/dataset_training.csv"
    validation_dataset_path = "../data/output/dataset/dataset_validation.csv"
    testing_dataset_path = "../data/output/dataset/dataset_testing.csv"

    training_dataset = MultiLabelHateSpeechClassificationDataset(
        training_dataset_path, tokenizer, lowercase=True
    )
    validation_dataset = MultiLabelHateSpeechClassificationDataset(
        validation_dataset_path, tokenizer, lowercase=True
    )
    testing_dataset = MultiLabelHateSpeechClassificationDataset(
        testing_dataset_path, tokenizer, lowercase=True
    )

    training_loader = HateSpeechClassificationDataLoader(
        dataset=training_dataset,
        max_seq_len=max_seq_len,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=True,
    )
    validation_loader = HateSpeechClassificationDataLoader(
        dataset=validation_dataset,
        max_seq_len=max_seq_len,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )
    testing_loader = HateSpeechClassificationDataLoader(
        dataset=testing_dataset,
        max_seq_len=max_seq_len,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )

    print(
        "\nModel will be trained with {} training data, {} validation data, and {} testing data".format(
            len(training_dataset), len(validation_dataset), len(testing_dataset)
        )
    )
    print(
        "Model will be trained with {} training dataloader, {} validation dataloader, and {} testing dataloader".format(
            len(training_loader), len(validation_loader), len(testing_loader)
        )
    )

    """Labeling the index and vice versa"""

    w2i, i2w = (
        MultiLabelHateSpeechClassificationDataset.LABEL2INDEX,
        MultiLabelHateSpeechClassificationDataset.INDEX2LABEL,
    )

    print("\n--- Label and Index Classification ---")
    print(w2i)
    print(i2w)

    """Classification only with IndoBERT pre-trained model."""

    print("\nApplying classification with IndoBERT pre-trained model...")

    text = "kebahagiaan terbesarku adalah melihatmu bersama dengan dirinya"
    subwords = tokenizer.encode(text)
    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)

    logits = model(subwords)[0]
    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()

    print(f"\nText: {text}")
    print(
        f"Label: {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)"
    )

    text = "tukang bully mati aja sana"
    subwords = tokenizer.encode(text)
    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)

    logits = model(subwords)[0]
    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()

    print(f"\nText: {text}")
    print(
        f"Label: {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)"
    )

    text = "antek antek komunis mati sana"
    subwords = tokenizer.encode(text)
    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)

    logits = model(subwords)[0]
    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()

    print(f"\nText: {text}")
    print(
        f"Label: {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)"
    )

    text = "kemaren gue ga di ajak tai emang"
    subwords = tokenizer.encode(text)
    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)

    logits = model(subwords)[0]
    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()

    print(f"\nText: {text}")
    print(
        f"Label: {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)"
    )

    """
    Fine Tuning.

    Now the input data: properly formatted, it's time to fine tune the BERT model.
    """

    print("\n[=== Fine Tuning ===]")
    print("\nApplying AdamW Optimizer...")
    for i in tqdm(range(0, 100), desc="Applying"):
        time.sleep(0.02)

    # Apply AdamW Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=lr, eps=eps, weight_decay=weight_decay
    )
    model = model.cuda()

    print("\n[AdamW Optimizer successfully applied]\n")

    """
    This training code: based on the `run_glue.py` script here:
    https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128
    """

    train_acc_lists = []
    train_pre_lists = []
    train_rec_lists = []
    train_f1_lists = []

    eval_acc_lists = []
    eval_pre_lists = []
    eval_rec_lists = []
    eval_f1_lists = []

    train_loss_lists = []
    eval_loss_lists = []

    print("Fine Tuning...\n")

    # Fine tuning
    for epoch in range(0, epochs):

        # ========================================
        #               Training
        # ========================================

        model.train()
        torch.set_grad_enabled(True)

        # Reset the total loss for this epoch
        total_train_loss = 0

        list_hyp, list_label = [], []

        train_pbar = tqdm(training_loader, leave=True, total=len(training_loader))
        for i, batch_data in enumerate(train_pbar):
            # Forward model
            loss, batch_hyp, batch_label = forward_sequence_classification(
                model, batch_data[:-1], i2w=i2w, device="cuda"
            )

            # Update model
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            tr_loss = loss.item()
            total_train_loss = total_train_loss + tr_loss

            # Calculate metrics
            list_hyp += batch_hyp
            list_label += batch_label

            train_pbar.set_description(
                "(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}".format(
                    (epoch + 1), total_train_loss / (i + 1), get_lr(optimizer)
                )
            )

        # Calculate train metric
        metrics = multi_label_hate_speech_classification_metrics_fn(
            list_hyp, list_label
        )
        print(
            "(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}".format(
                (epoch + 1),
                total_train_loss / (i + 1),
                metrics_to_string(metrics),
                get_lr(optimizer),
            )
        )

        train_acc_lists.append(metrics["ACC"])
        train_pre_lists.append(metrics["PRE"])
        train_rec_lists.append(metrics["REC"])
        train_f1_lists.append(metrics["F1"])
        current_train_loss = round(total_train_loss / (i + 1), 4)
        train_loss_lists.append(current_train_loss)

        # ========================================
        #               Validation
        # ========================================

        """
        After the completion of each training epoch, measure our performance on
        our validation set.
        """

        model.eval()
        torch.set_grad_enabled(False)

        total_loss, total_correct, total_labels = 0, 0, 0
        list_hyp, list_label = [], []

        pbar = tqdm(validation_loader, leave=True, total=len(validation_loader))

        for i, batch_data in enumerate(pbar):
            batch_seq = batch_data[-1]
            loss, batch_hyp, batch_label = forward_sequence_classification(
                model, batch_data[:-1], i2w=i2w, device="cuda"
            )

            # Calculate total loss
            valid_loss = loss.item()
            total_loss = total_loss + valid_loss

            # Calculate evaluation metrics
            list_hyp += batch_hyp
            list_label += batch_label
            metrics = multi_label_hate_speech_classification_metrics_fn(
                list_hyp, list_label
            )

            pbar.set_description(
                "(Epoch {}) VALID LOSS:{:.4f} {}".format(
                    (epoch + 1), total_loss / (i + 1), metrics_to_string(metrics)
                )
            )

        metrics = multi_label_hate_speech_classification_metrics_fn(
            list_hyp, list_label
        )
        print(
            "(Epoch {}) VALID LOSS:{:.4f} {}".format(
                (epoch + 1), total_loss / (i + 1), metrics_to_string(metrics)
            )
        )

        eval_acc_lists.append(metrics["ACC"])
        eval_pre_lists.append(metrics["PRE"])
        eval_rec_lists.append(metrics["REC"])
        eval_f1_lists.append(metrics["F1"])
        current_eval_loss = round(total_loss / (i + 1), 4)
        eval_loss_lists.append(current_eval_loss)

    # Show line chart for Training and Validation Accuracy, Precision, Recall, and F1-score
    fig, (plt1, plt2, plt3, plt4) = plt.subplots(1, 4, figsize=(30, 7))
    epoch = [1]

    plt1.plot(epoch, train_acc_lists, label="train")
    plt1.plot(epoch, eval_acc_lists, label="valid")
    plt1.set_title("Training and Validation Accuracy")
    plt1.legend()
    plt1.set(xlabel="Epoch", ylabel="Accuracy")

    plt2.plot(epoch, train_pre_lists, label="train")
    plt2.plot(epoch, eval_pre_lists, label="valid")
    plt2.set_title("Training and Validation Precision")
    plt2.legend()
    plt2.set(xlabel="Epoch", ylabel="Precision")

    plt3.plot(epoch, train_rec_lists, label="train")
    plt3.plot(epoch, eval_rec_lists, label="valid")
    plt3.set_title("Training and Validation Recall")
    plt3.legend()
    plt3.set(xlabel="Epoch", ylabel="Recall")

    plt4.plot(epoch, train_f1_lists, label="train")
    plt4.plot(epoch, eval_f1_lists, label="valid")
    plt4.set_title("Training and Validation F1-score")
    plt4.legend()
    plt4.set(xlabel="Epoch", ylabel="F1-score")

    # Show line chart for Training and Validation Loss
    fig, plt5 = plt.subplots(1, 1, figsize=(6, 6))
    epoch = [1]

    plt5.plot(epoch, train_loss_lists, label="train")
    plt5.plot(epoch, eval_loss_lists, label="valid")
    plt5.set_title("Training and Validation Loss")
    plt5.legend()
    plt5.set(xlabel="Epoch", ylabel="Accuracy")

    # If you don't want to save these charts, comment lines below
    print("\nSaving training result charts...")
    for i in tqdm(range(0, 100), desc="Saving"):
        time.sleep(0.02)

    plt1.figure.savefig("../data/output/image/training_validation_metrics", dpi=300)
    plt5.figure.savefig("../data/output/image/training_validation_loss", dpi=300)

    print("\n[Charts successfully downloaded]")

    # Constructing dataframe from training
    df_stats = {
        "Train Accuracy": np.round(train_acc_lists, 2),
        "Train Precision": np.round(train_pre_lists, 2),
        "Train Recall": np.round(train_rec_lists, 2),
        "Train F1": np.round(train_f1_lists, 2),
        "Valid Accuracy": np.round(eval_acc_lists, 2),
        "Valid Precision": np.round(eval_pre_lists, 2),
        "Valid Recall": np.round(eval_rec_lists, 2),
        "Valid F1": np.round(eval_f1_lists, 2),
        "Train Loss": np.round(train_loss_lists, 2),
        "Valid Loss": np.round(eval_loss_lists, 2),
    }

    # Create a dataframe from training
    df_stats = pd.DataFrame(data=df_stats)

    # Rename row index as 'Epoch'
    df_stats = df_stats.rename_axis("Epoch")

    # Display the table
    df_stats.index = df_stats.index + 1
    print("\n--- Training and Validation Statistics Table ---")
    print(df_stats)


def evaluate_model(i2w, model, tokenizer, testing_loader):
    """
    Evaluate on Test Set.
    With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set.
    """

    # Put model in evaluation mode
    model.eval()
    torch.set_grad_enabled(False)

    total_loss, total_correct, total_labels = 0, 0, 0
    list_hyp, list_label = [], []

    print("Evaluate the model on Test Set")
    print("\nLoading Testing DataLoader...")

    pbar = tqdm(testing_loader, leave=True, total=len(testing_loader))

    for i, batch_data in enumerate(pbar):
        _, batch_hyp, _ = forward_sequence_classification(
            model, batch_data[:-1], i2w=i2w, device="cuda"
        )
        list_hyp += batch_hyp
    print("\n[Testing DataLoader successfully loaded]")

    df = pd.DataFrame({"label": list_hyp}).reset_index()
    df.to_csv("../models/prediction.txt", index=False)

    print("\n--- Prediction Table ---")
    print(df)

    print("\nEvaluating...\n")

    prediction = pd.read_csv("../models/prediction.txt")

    prediction_list = []

    for i in prediction["label"]:
        if i == "HS_Weak":
            prediction_list.append(1)
        elif i == "HS_Moderate":
            prediction_list.append(2)
        elif i == "HS_Strong":
            prediction_list.append(3)
        else:
            prediction_list.append(0)

    data_test = pd.read_csv("../data/output/dataset/dataset_testing.csv")

    def label_classification(hs):
        label = ""

        if str(hs) == "HS_Weak":
            label = 1
        elif str(hs) == "HS_Moderate":
            label = 2
        elif str(hs) == "HS_Strong":
            label = 3
        else:
            label = 0

        return label

    data_test["label"] = data_test["label"].apply(label_classification)
    data_test = data_test[["text", "label"]]

    print("--- Result of classification ---\n")
    print("==================================")

    # Accuracy, precision, and recall
    print(
        f"Accuracy: {round(accuracy_score(data_test['label'], prediction_list) * 100)}%"
    )
    print(
        f"Precision: {round(precision_score(data_test['label'], prediction_list, average='macro') * 100)}%"
    )
    print(
        f"Recall: {round(recall_score(data_test['label'], prediction_list, average='macro') * 100)}%"
    )

    # Micro avg of f1-score
    f1_score_micro = (
        f1_score(data_test["label"], prediction_list, average="micro") * 100
    )
    print("F1-score (Micro Avg): {}%".format(round(f1_score_micro)))

    # Macro avg of f1-score
    f1_score_macro = (
        f1_score(data_test["label"], prediction_list, average="macro") * 100
    )
    print("F1-score (Macro Avg): {}%".format(round(f1_score_macro)))

    # Weighted avg of f1-score
    f1_score_weighted = (
        f1_score(data_test["label"], prediction_list, average="weighted") * 100
    )
    print("F1-score (Weighted Avg): {}%".format(round(f1_score_weighted)))

    print("==================================")

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(data_test["label"], prediction_list))

    report = classification_report(
        data_test["label"], prediction_list, output_dict=True
    )
    report = pd.DataFrame(report).transpose()
    report.to_csv("../data/output/result/classification_report.csv")

    # Confusion Matrix
    f, ax = plt.subplots(figsize=(10, 5))
    sns.heatmap(
        confusion_matrix(data_test["label"], prediction_list),
        annot=True,
        fmt=".0f",
        ax=ax,
    )
    plt.xlabel("True Label")
    plt.ylabel("Predicted Label")

    plt.savefig("../data/output/result/confusion_matrix", dpi=300)

    print("\nDownloading Classification Report and Confusion Matrix...")
    for i in tqdm(range(0, 100), desc="Downloading"):
        time.sleep(0.05)
    print("\n[Classification Report and Confusion Matrix successfully downloaded]")

    print("\n[================================================]\n")

    """
    Compare True and Prediction Result.
    True and prediction result should at least have the similar or same result.
    """

    print("Comparing true and prediction result...\n")

    # True label
    amount_of_true_non_hs = 0
    amount_of_true_hs_weak = 0
    amount_of_true_hs_moderate = 0
    amount_of_true_hs_strong = 0

    for i in data_test["label"]:
        if i == 1:
            amount_of_true_hs_weak += 1
        elif i == 2:
            amount_of_true_hs_moderate += 1
        elif i == 3:
            amount_of_true_hs_strong += 1
        else:
            amount_of_true_non_hs += 1

    # Prediction label
    amount_of_pred_non_hs = 0
    amount_of_pred_hs_weak = 0
    amount_of_pred_hs_moderate = 0
    amount_of_pred_hs_strong = 0

    for i in prediction_list:
        if i == 1:
            amount_of_pred_hs_weak += 1
        elif i == 2:
            amount_of_pred_hs_moderate += 1
        elif i == 3:
            amount_of_pred_hs_strong += 1
        else:
            amount_of_pred_non_hs += 1

    print(
        "Non-Hate Speech count: True {} and Prediction {}".format(
            amount_of_true_non_hs, amount_of_pred_non_hs
        )
    )
    print(
        "Weak Hate Speech count: True {} and Prediction {}".format(
            amount_of_true_hs_weak, amount_of_pred_hs_weak
        )
    )
    print(
        "Moderate Hate Speech count: True {} and Prediction {}".format(
            amount_of_true_hs_moderate, amount_of_pred_hs_moderate
        )
    )
    print(
        "Strong Hate Speech count: True {} and Prediction {}".format(
            amount_of_true_hs_strong, amount_of_pred_hs_strong
        )
    )

    """Show compare result on chart."""

    fig, (ax1, ax2) = plt.subplots(
        1, 2, figsize=(20, 10), subplot_kw=dict(aspect="equal")
    )
    labels = "Non_HS", "HS_Weak", "HS_Moderate", "HS_Strong"
    labels = [x.split()[-1] for x in labels]

    # True result
    sizes = [
        amount_of_true_non_hs,
        amount_of_true_hs_weak,
        amount_of_true_hs_moderate,
        amount_of_true_hs_strong,
    ]
    explode = (0.1, 0, 0, 0)

    ax1.pie(
        sizes,
        explode=explode,
        labels=labels,
        autopct="%1.1f%%",
        shadow=True,
        startangle=90,
        textprops=dict(color="white", weight="bold"),
    )
    ax1.axis("equal")
    ax1.set_title("Labeling True Result")

    # Prediction result
    sizes = [
        amount_of_pred_non_hs,
        amount_of_pred_hs_weak,
        amount_of_pred_hs_moderate,
        amount_of_pred_hs_strong,
    ]
    explode = (0.1, 0, 0, 0)

    ax2.pie(
        sizes,
        explode=explode,
        labels=labels,
        autopct="%1.1f%%",
        shadow=True,
        startangle=90,
        textprops=dict(color="white", weight="bold"),
    )
    ax2.axis("equal")
    ax2.set_title("Labeling Prediction Result")

    # Set legend label
    ax1.legend(labels, title="Label", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

    ax1.plot()
    ax2.plot()

    plt.savefig("../data/output/image/prediction_result", dpi=300)
    print("\n[Result chart successfully downloaded]")

    """Test Fine-Tuned Model."""

    print("\n[================================================]\n")
    print("Applying classification with trained model...")

    text = "kebahagiaan terbesarku adalah melihatmu bersama dengan dirinya"
    subwords = tokenizer.encode(text)
    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)

    logits = model(subwords)[0]
    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()

    print(f"\nText: {text}")
    print(
        f"Label: {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)"
    )

    text = "tukang bully mati aja sana"
    subwords = tokenizer.encode(text)
    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)

    logits = model(subwords)[0]
    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()

    print(f"\nText: {text}")
    print(
        f"Label: {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)"
    )

    text = "antek antek komunis mati sana"
    subwords = tokenizer.encode(text)
    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)

    logits = model(subwords)[0]
    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()

    print(f"\nText: {text}")
    print(
        f"Label: {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)"
    )

    text = "kemaren gue ga di ajak tai emang"
    subwords = tokenizer.encode(text)
    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)

    logits = model(subwords)[0]
    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()

    print(f"\nText: {text}")
    print(
        f"Label: {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)"
    )

    print("\n--- PyTorch and TensorFlow info ---")
    print(logits)
    print(torch.topk(logits, 1))


def save_model(model, tokenizer):
    """
    Save Trained Model.

    To save your model, download it to your device.
    """

    path = "../models"

    # check whether the specified path exists or not
    # if not exists, path will be created
    if not os.path.exists(path):
        # create output model path
        os.makedirs(path)

    print("Saving trained model to %s..." % path)

    """
    Save a trained model, configuration and tokenizer using `save_pretrained()`.
    They can then be reloaded using `from_pretrained()`.
    """

    model_to_save = model.module if hasattr(model, "module") else model
    model_to_save.save_pretrained(path)
    tokenizer.save_pretrained(path)

    # save tokenizer
    with open("../models/tokenizer.pkl", "wb") as handle:
        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

    for i in tqdm(range(0, 100), desc="Saving"):
        time.sleep(0.02)

    print("\n[Trained model successfully downloaded]\n")


def elapsed_time(end_time):
    """
    Show the elapsed time of this program.
    """

    mins = str((end_time % 3600) // 60)
    secs = str((end_time % 3600) % 60)
    time = "--- Program executed in {} minutes and {} seconds ---".format(mins, secs)

    return print(time)


def main():
    """Main app."""

    start_time = timeit.default_timer()

    # Check Tensorflow and Pytorch versions
    print("Running with...")
    print("TensorFlow version: " + tf.__version__)
    print("Pytorch version: " + torch.__version__ + "\n")

    """
    In order for torch to use the GPU, we need to identify and specify
    the GPU as the device. Later, in our training loop,
    we will load data onto the device.
    """

    # If there's a GPU available
    if torch.cuda.is_available():
        # Tell PyTorch to use the GPU.
        device = torch.device("cuda")

        # Get the GPU device name
        device_name = tf.test.gpu_device_name()

        print("\nThere are %d GPU(s) available" % torch.cuda.device_count())
        print("Found GPU at: {}".format(device_name))
        print("GPU:", torch.cuda.get_device_name(0))

    # If not
    else:
        print("No GPU available.")
        device = torch.device("cpu")

    print("\n[================================================]")

    # Set the seed value all over the place to make this reproducible
    set_seed(0)

    """
    Import dataset
    Forked from 'https://raw.githubusercontent.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection/master/re_dataset.csv (Ibrohim & Budi, 2019).
    """

    print("\nImporting dataset...")
    for i in tqdm(range(0, 100), desc="Importing"):
        time.sleep(0.02)

    df = "https://raw.githubusercontent.com/ryzanugrah/id-multi-label-hate-speech-and-abusive-language-detection/master/re_multi_label_dataset.csv"
    df = pd.read_csv(df, encoding="utf-8")

    print("\n[Dataset successfully imported]\n")

    # Check how many records are in the dataset
    df.info()

    # Show the first 5 dataset rows
    print("\n--- Multi Label Hate Speech Dataset ---")
    print(df.head())
    print("Number of Data: ", df.shape)
    print("\n[================================================]\n")

    """Show Data on Chart."""

    # Histogram
    show_histogram(df)

    # Donut chart
    # Count value on each label
    non_hs_label = df.loc[df.HS == 0, "HS"].count()
    hs_label = df.loc[df.HS == 1, "HS"].count()
    hs_weak_label = df.loc[df.HS_Weak == 1, "HS_Weak"].count()
    hs_moderate_label = df.loc[df.HS_Moderate == 1, "HS_Moderate"].count()
    hs_strong_label = df.loc[df.HS_Strong == 1, "HS_Strong"].count()

    inner_label = ["Non_HS", "HS_Weak", "HS_Moderate", "HS_Strong"]
    outer_label = ["Non_HS", "HS"]
    count_inner_data = [non_hs_label, hs_weak_label, hs_moderate_label, hs_strong_label]
    count_outer_data = [non_hs_label, hs_label]

    show_pie(inner_label, outer_label, count_inner_data, count_outer_data, "Label")
    print("\n[Chart successfully downloaded]")
    print("\n[================================================]")

    text_preprocessing(df)
    print("\n[================================================]\n")

    index_classification(df)
    print("\n[================================================]\n")

    """Remove Missing Values."""

    print("Removing missing values...")

    df_indexed = pd.read_csv("../data/output/dataset/dataset_preprocessed.csv")
    print("\n--- Before removing missing values ---")
    print(df_indexed.isnull().sum())

    df_indexed = df_indexed[df_indexed["text"].notna()]
    print("\n--- After removing missing values ---")
    print(df_indexed.isnull().sum())

    print("Number of Indexed Dataset: ", df_indexed.shape)

    # Save dataset
    print("\nSaving Preprocessed Dataset...")
    df_indexed.to_csv("../data/output/dataset/dataset_preprocessed.csv", index=False)
    print("\n[Preprocessed Dataset successfully saved]")

    print("\n[================================================]\n")

    split_dataset()
    print("\n[================================================]\n")

    fine_tuning()
    print("\n[================================================]\n")

    evaluate_model(i2w, model, tokenizer, testing_loader)
    print("\n[================================================]\n")

    save_model(model, tokenizer)

    stop_time = timeit.default_timer()
    end_time = int(round((stop_time - start_time)))

    elapsed_time(end_time)


"""
Using the special variable
__name__
to run main function
"""

if __name__ == "__main__":
    main()
